{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1\n",
    "#### Name: Serena Yang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. [40 pts] Inverted Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'#1': 'Glimpse is an index and query system that allow for search through a file system or document collection quickly. Glimpse is the default search engine in a large information retrieve system. It has also been use as part of some web search engine', '#2': 'The main process in an information retrieve system are document index, query process, query evaluation and relevance feedback. Among these, efficient update of the index is critical in large document collection', '#3': 'The retrieve model in a search engine determine a document relevance to a query. An example of a retrieve model is the vector space model in which document and query are represent as vector. Vector operation are use to determine similar between query and document', '#4': 'Cluster is the task of group similar object. In information retrieve, cluster can be use to find document with similar content or to group together term that appear frequent together in document', '#5': 'Cluster are create from short snippet of document retrieve by web search engine which are as good as cluster create from the full text of web document'}\n"
     ]
    }
   ],
   "source": [
    "#take a set of documents as input\n",
    "import urllib\n",
    "\n",
    "url = 'http://facweb.cs.depaul.edu/mobasher/classes/csc575/Assignments/hw1-test-docs.txt'\n",
    "file = urllib.request.urlopen(url)\n",
    "temp = {}\n",
    "for line in file:\n",
    "    decoded_line = line.decode(\"utf-8\")\n",
    "    if decoded_line[0] == '#':\n",
    "        index = decoded_line[:2]\n",
    "    elif decoded_line == '\\r\\n':\n",
    "        pass\n",
    "    else:\n",
    "        l = len(decoded_line)\n",
    "        temp[index] = decoded_line[:l-3]\n",
    "    \n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst', 'amount', 'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'around', 'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'bill', 'both', 'bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant', 'co', 'con', 'could', 'couldnt', 'cry', 'de', 'describe', 'detail', 'do', 'done', 'down', 'due', 'during', 'each', 'eg', 'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'etc', 'even', 'ever', 'every', 'everyone', 'everything', 'everywhere', 'except', 'few', 'fifteen', 'fify', 'fill', 'find', 'fire', 'first', 'five', 'for', 'former', 'formerly', 'forty', 'found', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had', 'has', 'hasnt', 'have', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed', 'interest', 'into', 'is', 'it', 'its', 'itself', 'keep', 'last', 'latter', 'latterly', 'least', 'less', 'ltd', 'made', 'many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine', 'more', 'moreover', 'most', 'mostly', 'move', 'much', 'must', 'my', 'myself', 'name', 'namely', 'neither', 'never', 'nevertheless', 'next', 'nine', 'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of', 'off', 'often', 'on', 'once', 'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'part', 'per', 'perhaps', 'please', 'put', 'rather', 're', 'same', 'see', 'seem', 'seemed', 'seeming', 'seems', 'serious', 'several', 'she', 'should', 'show', 'side', 'since', 'sincere', 'six', 'sixty', 'so', 'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'such', 'system', 'take', 'ten', 'than', 'that', 'the', 'their', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'thick', 'thin', 'third', 'this', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'top', 'toward', 'towards', 'twelve', 'twenty', 'two', 'un', 'under', 'until', 'up', 'upon', 'us', 'very', 'via', 'was', 'we', 'well', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yet', 'you', 'your', 'yours', 'yourself', 'yourselves']\n"
     ]
    }
   ],
   "source": [
    "#make stop words dictionary\n",
    "stopwords = []\n",
    "url = 'http://facweb.cs.depaul.edu/mobasher/classes/csc575/Assignments/stopwords_en.txt'\n",
    "file = urllib.request.urlopen(url)\n",
    "for line in file:\n",
    "    decoded_line = line.decode(\"utf-8\")\n",
    "    stopwords.append(decoded_line.strip('\\n'))\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/serenayang/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'#1': ['glimpse', 'index', 'query', 'allow', 'search', 'file', 'document', 'collection', 'quickly', 'glimpse', 'default', 'search', 'engine', 'large', 'information', 'retrieve', 'use', 'web', 'search', 'engine'], '#2': ['main', 'process', 'information', 'retrieve', 'document', 'index', 'query', 'process', 'query', 'evaluation', 'relevance', 'feedback', 'efficient', 'update', 'index', 'critical', 'large', 'document', 'collection'], '#3': ['retrieve', 'model', 'search', 'engine', 'determine', 'document', 'relevance', 'query', 'example', 'retrieve', 'model', 'vector', 'space', 'model', 'document', 'query', 'represent', 'vector', 'vector', 'operation', 'use', 'determine', 'similar', 'query', 'document'], '#4': ['cluster', 'task', 'group', 'similar', 'object', 'information', 'retrieve', 'cluster', 'use', 'document', 'similar', 'content', 'group', 'term', 'appear', 'frequent', 'document'], '#5': ['cluster', 'create', 'short', 'snippet', 'document', 'retrieve', 'web', 'search', 'engine', 'good', 'cluster', 'create', 'text', 'web', 'document']}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = {}\n",
    "test = []\n",
    "punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "for key, value in temp.items():\n",
    "    #parse each document to identify tokens (tokenization)\n",
    "    words = word_tokenize(temp[key])\n",
    "    \n",
    "    #convert tokens to lower case, remove punctuation and stop words\n",
    "    for word in words:\n",
    "        #remove punctuation\n",
    "        if word in punc:\n",
    "            pass\n",
    "        else:\n",
    "            #convert tokens to lower case\n",
    "            lowword = word.lower()\n",
    "            #remove stop words\n",
    "            if lowword not in stopwords:\n",
    "                test.append(lowword)\n",
    "            else:\n",
    "                pass\n",
    "    \n",
    "    tokens[key] = test\n",
    "    test = []\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "InvertedIndex, each, posting = {}, {}, {}\n",
    "temp = []\n",
    "for key, value in tokens.items():\n",
    "    for word in value:\n",
    "        if word not in InvertedIndex:\n",
    "            each[key] = 1\n",
    "            InvertedIndex[word] = each\n",
    "        else:\n",
    "            if key not in InvertedIndex[word]:\n",
    "                InvertedIndex[word][key] = 1\n",
    "            elif key in InvertedIndex[word]:\n",
    "                Df = InvertedIndex[word][key]\n",
    "                Df += 1\n",
    "                InvertedIndex[word][key] = Df\n",
    "            \n",
    "        each = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "each = {}\n",
    "count = 0\n",
    "for key, value in InvertedIndex.items():\n",
    "    for i in list(InvertedIndex[key]):\n",
    "        count += InvertedIndex[key][i]\n",
    "        \n",
    "    each['DocF'] = len(value)\n",
    "    each['TotalF'] = count\n",
    "    InvertedIndex[key]['Info'] = each\n",
    "    count = 0\n",
    "    each = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'glimpse': {'#1': 2, 'Info': {'DocF': 1, 'TotalF': 2}},\n",
       " 'index': {'#1': 1, '#2': 2, 'Info': {'DocF': 2, 'TotalF': 3}},\n",
       " 'query': {'#1': 1, '#2': 2, '#3': 3, 'Info': {'DocF': 3, 'TotalF': 6}},\n",
       " 'allow': {'#1': 1, 'Info': {'DocF': 1, 'TotalF': 1}},\n",
       " 'search': {'#1': 3, '#3': 1, '#5': 1, 'Info': {'DocF': 3, 'TotalF': 5}},\n",
       " 'file': {'#1': 1, 'Info': {'DocF': 1, 'TotalF': 1}},\n",
       " 'document': {'#1': 1,\n",
       "  '#2': 2,\n",
       "  '#3': 3,\n",
       "  '#4': 2,\n",
       "  '#5': 2,\n",
       "  'Info': {'DocF': 5, 'TotalF': 10}},\n",
       " 'collection': {'#1': 1, '#2': 1, 'Info': {'DocF': 2, 'TotalF': 2}},\n",
       " 'quickly': {'#1': 1, 'Info': {'DocF': 1, 'TotalF': 1}},\n",
       " 'default': {'#1': 1, 'Info': {'DocF': 1, 'TotalF': 1}},\n",
       " 'engine': {'#1': 2, '#3': 1, '#5': 1, 'Info': {'DocF': 3, 'TotalF': 4}},\n",
       " 'large': {'#1': 1, '#2': 1, 'Info': {'DocF': 2, 'TotalF': 2}},\n",
       " 'information': {'#1': 1, '#2': 1, '#4': 1, 'Info': {'DocF': 3, 'TotalF': 3}},\n",
       " 'retrieve': {'#1': 1,\n",
       "  '#2': 1,\n",
       "  '#3': 2,\n",
       "  '#4': 1,\n",
       "  '#5': 1,\n",
       "  'Info': {'DocF': 5, 'TotalF': 6}},\n",
       " 'use': {'#1': 1, '#3': 1, '#4': 1, 'Info': {'DocF': 3, 'TotalF': 3}},\n",
       " 'web': {'#1': 1, '#5': 2, 'Info': {'DocF': 2, 'TotalF': 3}},\n",
       " 'main': {'#2': 1, 'Info': {'DocF': 1, 'TotalF': 1}},\n",
       " 'process': {'#2': 2, 'Info': {'DocF': 1, 'TotalF': 2}},\n",
       " 'evaluation': {'#2': 1, 'Info': {'DocF': 1, 'TotalF': 1}},\n",
       " 'relevance': {'#2': 1, '#3': 1, 'Info': {'DocF': 2, 'TotalF': 2}},\n",
       " 'feedback': {'#2': 1, 'Info': {'DocF': 1, 'TotalF': 1}},\n",
       " 'efficient': {'#2': 1, 'Info': {'DocF': 1, 'TotalF': 1}},\n",
       " 'update': {'#2': 1, 'Info': {'DocF': 1, 'TotalF': 1}},\n",
       " 'critical': {'#2': 1, 'Info': {'DocF': 1, 'TotalF': 1}},\n",
       " 'model': {'#3': 3, 'Info': {'DocF': 1, 'TotalF': 3}},\n",
       " 'determine': {'#3': 2, 'Info': {'DocF': 1, 'TotalF': 2}},\n",
       " 'example': {'#3': 1, 'Info': {'DocF': 1, 'TotalF': 1}},\n",
       " 'vector': {'#3': 3, 'Info': {'DocF': 1, 'TotalF': 3}},\n",
       " 'space': {'#3': 1, 'Info': {'DocF': 1, 'TotalF': 1}},\n",
       " 'represent': {'#3': 1, 'Info': {'DocF': 1, 'TotalF': 1}},\n",
       " 'operation': {'#3': 1, 'Info': {'DocF': 1, 'TotalF': 1}},\n",
       " 'similar': {'#3': 1, '#4': 2, 'Info': {'DocF': 2, 'TotalF': 3}},\n",
       " 'cluster': {'#4': 2, '#5': 2, 'Info': {'DocF': 2, 'TotalF': 4}},\n",
       " 'task': {'#4': 1, 'Info': {'DocF': 1, 'TotalF': 1}},\n",
       " 'group': {'#4': 2, 'Info': {'DocF': 1, 'TotalF': 2}},\n",
       " 'object': {'#4': 1, 'Info': {'DocF': 1, 'TotalF': 1}},\n",
       " 'content': {'#4': 1, 'Info': {'DocF': 1, 'TotalF': 1}},\n",
       " 'term': {'#4': 1, 'Info': {'DocF': 1, 'TotalF': 1}},\n",
       " 'appear': {'#4': 1, 'Info': {'DocF': 1, 'TotalF': 1}},\n",
       " 'frequent': {'#4': 1, 'Info': {'DocF': 1, 'TotalF': 1}},\n",
       " 'create': {'#5': 2, 'Info': {'DocF': 1, 'TotalF': 2}},\n",
       " 'short': {'#5': 1, 'Info': {'DocF': 1, 'TotalF': 1}},\n",
       " 'snippet': {'#5': 1, 'Info': {'DocF': 1, 'TotalF': 1}},\n",
       " 'good': {'#5': 1, 'Info': {'DocF': 1, 'TotalF': 1}},\n",
       " 'text': {'#5': 1, 'Info': {'DocF': 1, 'TotalF': 1}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "InvertedIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Show the full contents of the dictionary in lexicographic order of tokens along with document frequencies and total frequencies (in an easily readable format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IndexTerms      DocFreq    TotalFreq \n",
      "allow           1          1         \n",
      "appear          1          1         \n",
      "cluster         2          4         \n",
      "collection      2          2         \n",
      "content         1          1         \n",
      "create          1          2         \n",
      "critical        1          1         \n",
      "default         1          1         \n",
      "determine       1          2         \n",
      "document        5          10        \n",
      "efficient       1          1         \n",
      "engine          3          4         \n",
      "evaluation      1          1         \n",
      "example         1          1         \n",
      "feedback        1          1         \n",
      "file            1          1         \n",
      "frequent        1          1         \n",
      "glimpse         1          2         \n",
      "good            1          1         \n",
      "group           1          2         \n",
      "index           2          3         \n",
      "information     3          3         \n",
      "large           2          2         \n",
      "main            1          1         \n",
      "model           1          3         \n",
      "object          1          1         \n",
      "operation       1          1         \n",
      "process         1          2         \n",
      "query           3          6         \n",
      "quickly         1          1         \n",
      "relevance       2          2         \n",
      "represent       1          1         \n",
      "retrieve        5          6         \n",
      "search          3          5         \n",
      "short           1          1         \n",
      "similar         2          3         \n",
      "snippet         1          1         \n",
      "space           1          1         \n",
      "task            1          1         \n",
      "term            1          1         \n",
      "text            1          1         \n",
      "update          1          1         \n",
      "use             3          3         \n",
      "vector          1          3         \n",
      "web             2          3         \n"
     ]
    }
   ],
   "source": [
    "print(\"{:<15} {:<10} {:<10}\".format('IndexTerms', 'DocFreq', 'TotalFreq'))\n",
    "for key in sorted(InvertedIndex.keys()):\n",
    "    a = InvertedIndex[key]['Info']['DocF']\n",
    "    b = InvertedIndex[key]['Info']['TotalF']\n",
    "    print(\"{:<15} {:<10} {:<10}\".format(key, a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Show the output of your program when searching the postings for the tokens: \"index\", \"cluster\", \"query\", \"search\", \"engine\", and \"retrieve\" (in each case the output should show the \"hit list\" of documents containing the token along with the token's occurrence count in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['index', 'cluster', 'query', 'search', 'engine', 'retrieve']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q =[\"index\", \"cluster\", \"query\", \"search\", \"engine\", \"retrieve\"]\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index {'#1': 1, '#2': 2}\n",
      "cluster {'#4': 2, '#5': 2}\n",
      "query {'#1': 1, '#2': 2, '#3': 3}\n",
      "search {'#1': 3, '#3': 1, '#5': 1}\n",
      "engine {'#1': 2, '#3': 1, '#5': 1}\n",
      "retrieve {'#1': 1, '#2': 1, '#3': 2, '#4': 1, '#5': 1}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in Q:\n",
    "    if i in InvertedIndex:\n",
    "        row = InvertedIndex[i]\n",
    "        del row['Info']\n",
    "        print(i, row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.[25 pts] Character Ngrams to Find Similar Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(word, N):\n",
    "    arry = []\n",
    "    for i in range(len(word)):\n",
    "        temp = word[i:i+N]\n",
    "        if len(temp) == N:\n",
    "            arry.append(temp)\n",
    "        else:\n",
    "            pass\n",
    "    return arry\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_sim(word1, word2, N):\n",
    "    arry1, arry2 = [],[]\n",
    "    comuniq = 0\n",
    "    arry1 = set(split(word1, N))\n",
    "    arry2 = set(split(word2, N))\n",
    "    uniq1 = len(arry1)\n",
    "    uniq2 = len(arry2)\n",
    "    \n",
    "    for i in arry1:\n",
    "        if i in arry2:\n",
    "            comuniq += 1\n",
    "     \n",
    "    similarity = (2*comuniq)/(uniq1+uniq2)\n",
    "    print('Unique ngrams for word',word1, 'with n =', N, ':\\n', arry1)\n",
    "    print('Unique ngrams for word',word2, 'with n =', N, ':\\n', arry2)\n",
    "    print('Ngram similarity for:', word1, 'and', word2, 'using N =', N, ':', similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique ngrams for word informational with n = 2 :\n",
      " {'fo', 'or', 'at', 'rm', 'al', 'nf', 'na', 'ti', 'in', 'on', 'io', 'ma'}\n",
      "Unique ngrams for word formulation with n = 2 :\n",
      " {'fo', 'mu', 'or', 'at', 'la', 'rm', 'ti', 'on', 'io', 'ul'}\n",
      "Ngram similarity for: informational and formulation using N = 2 : 0.6363636363636364\n"
     ]
    }
   ],
   "source": [
    "word1 = 'informational'\n",
    "word2 = 'formulation'\n",
    "N = 2\n",
    "ngram_sim(word1, word2, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
